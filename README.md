<h1 align="center">
  Eric Transformer
</h1>

<p align="center">
  <a href="https://opensource.org/licenses/Apache-2.0">
    <img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" alt="License: Apache-2.0" height="20">
  </a>

</p>

<p align="center">
  <strong><a href="https://ericfillion.github.io/erictransformer/">https://ericfillion.github.io/erictransformer/</a></strong>
</p>


Local pre-training, fine-tuning and inference for LLMs. 

- Format your text data in JSONL and then use a few lines of code to train models. 
- Full-parameter training of GPT-OSS-20b on a single H200.
- Use Apple's new MLX-LM framework for fast inference. Run GPT-OSS-120b locally. 
- Enable RAG powered by [Eric Search](https://github.com/EricFillion/ericsearch).
- Local experiment tracking that displays charts and metrics. 


## Install
```sh
pip install erictransformer
```

[Documentation](https://ericfillion.github.io/erictransformer)

## Maintainers
- [Eric Fillion](https://github.com/ericfillion) Lead Maintainer
- [Ted Brownlow](https://github.com/ted537) Maintainer 


## Contributing 
We are currently not accepting contributions. 
